<title>CS140 Lecture notes -- Big-O</title>
<body bgcolor=ffffff>
<h1>CS140 Lecture notes -- Big-O</h1>
<LI><a href=http://web.eecs.utk.edu/~jplank>Jim Plank</a>
<LI>Directory: <b>/home/jplank/cs140/notes/BigO</b>
<LI>Lecture notes:
    <a href=http://web.eecs.utk.edu/~jplank/plank/classes/cs140/Fall-2004/notes/BigO/>
    <b>http://web.eecs.utk.edu/~jplank/plank/classes/cs140/Fall-2004/notes/BigO/</b></a>
<LI>
Original Notes: <i>
Wed Sep 22 11:48:08 EDT 2004
</i>
<LI>
Last Modification: <i>
Thu Oct  3 13:36:31 EDT 2019
</i>


<hr>
<h2>Big-O</h2>

Big-O notation is one of the ways in which we talk about how complex
an algorithm or program is.  It gives us a nice way of quantifying or 
classifying how fast or slow a program is as a function of the size
of its input, and independently of the machine on which it runs.
<p>
<h2>Examples</h2>

Let's look at a program (in <b><a href=src/linear1.cpp>src/linear1.cpp</a></b>):

<p><center><table border=3 cellpadding=3><td><pre>
<font color=blue>/* This program takes a number n on standard input.  
   It then executes a for loop that iterates n times, counting the iterations.  
   It prints the number of iterations and a timing that uses the system call gettimeofday().  
   That's why you need to include &lt;sys/time.h&gt;. */</font>

#include &lt;sys/time.h&gt;
#include &lt;cstdio&gt;
#include &lt;iostream&gt;
using namespace std;

int main()
{
  long long n, count, i;
  double start_time, end_time;
  struct timeval tv;

  if (!(cin &gt;&gt; n)) return 1;

  <font color=blue>/* Get the starting time. */</font>

  gettimeofday(&tv, NULL);
  start_time = tv.tv_usec;
  start_time /= 1000000.0;
  start_time += tv.tv_sec;

  <font color=blue>/* Here's the loop, that executes n times. */</font>

  count = 0;
  for (i = 0; i &lt; n; i++) count++;

  <font color=blue>/* Get the ending time. */</font>

  gettimeofday(&tv, NULL);
  end_time = tv.tv_usec;
  end_time /= 1000000.0;
  end_time += tv.tv_sec;

  <font color=blue>/* Print N, the iterations, and the time. */</font>

  printf("N: %lld     Count: %lld    Time: %.3lf\n", n, count, end_time - start_time);
  return 0;
}
</pre></td></table></center><p>

Obviously, this is a simple program.  I don't want to go into <b>gettimeofday</b> too much.
It returns the value of a timer, which includes seconds and microseconds.  I convert that
to a double, so that we can print out the timing the for loop.

Suppose we run this program with varying values of <b>n</b>.
What do we expect?  Well, as <b>n</b> increases, so will
the <b>count</b>, and so will the running time of the program:
<p>
(This is on my Macbook, chunking along at 2.2 Ghz in 2019):

<pre>
UNIX> <font color=darkred><b>echo 100000 | bin/linear1</b></font>
N: 100000     Count: 100000    Time: 0.000
UNIX> <font color=darkred><b>echo 1000000 | bin/linear1</b></font>
N: 1000000     Count: 1000000    Time: 0.003
UNIX> <font color=darkred><b>echo 10000000 | bin/linear1</b></font>
N: 10000000     Count: 10000000    Time: 0.020
UNIX> <font color=darkred><b>echo 100000000 | bin/linear1</b></font>
N: 100000000     Count: 100000000    Time: 0.195
UNIX> <font color=darkred><b>echo 1000000000 | bin/linear1</b></font>
N: 1000000000     Count: 1000000000    Time: 2.021
UNIX> <font color=darkred><b></b></font>
</pre>

Just what you'd think.  The running time is roughly linear.

Now, look at six other programs below.  I will just show their loops:
<p>

<table border=7 cellpadding=3>
<tr>
<td valign=top>
<b><a href=src/linear2.cpp>src/linear2.cpp</a></b>:<br>
<pre>
<font color=blue>/* This loop executes 2n times. */</font>

count = 0;
for (i = 0; i &lt; 2*n; i++) count++;
</pre>
</td>
<td valign=top>
<a href=src/log.cpp><b>src/log.cpp</b></a>:<br>
<pre>
<font color=blue>/* This loop  executes log_2(n) times. */</font>

count = 0;
for (i = 1; i &lt; n; i *= 2) count++;
</pre>
</td>
<td valign=top>
<b><a href=src/nlogn.cpp>src/nlogn.cpp</a></b>:<br>
<pre>
<font color=blue>/* This loop executes n*log_2(n) times. */</font>

count = 0;
for (j = 0; j &lt; n; j++) {
  for (i = 1; i &lt; n; i *= 2) {
    count++;
  }
}
</pre></td>
</tr>

<tr>
<td valign=top>
<b><a href=src/nsquared.cpp>src/nsquared.cpp</a></b>:<br>
<pre>
<font color=blue>/* This loop executes n*n times. */</font>

count = 0;
for (j = 0; j &lt; n; j++) {
  for (i = 0; i &lt; n; i++) {
    count++;
  }
}
</pre>
</td>

<td valign=top>
<b><a href=src/all_i_j_pairs.cpp>src/all_i_j_pairs.cpp</a></b>:<br>
<pre>
<font color=blue>/* This loop executes (n + 1) * n / 2 times.
   It arises when you enumerate all (i,j) pairs
   such at 0 <= i < j < n. */</font>

count = 0;
for (j = 0; j &lt; n; j++) {
  for (i = 0; i &lt; j; i++) {
    count++;
  }
}
</pre>
</td> 
<td valign=top>
<b><a href=src/two_to_the_n.cpp>src/two_to_the_n.cpp</a></b>:<br>
<pre>
<font color=blue>/* This loop executes 2^n times. */</font>

count = 0;
for (i = 0; i &lt; (1LL &lt;&lt; n); i++) {
  count++;
}
</pre>
</td> 
</tr>
</table>

<p>
In each of the programs, I tell you how many times the loop executes in the comment.
That will be the final value of <b>count</b>.  Make sure you can calculate all of these
below -- it's an easy test question:

<pre>
UNIX> <font color=darkred><b>echo 16 | bin/linear1</b></font>
N: 16     Count: 16    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/linear2</b></font>
N: 16     Count: 32    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/log</b></font>
N: 16     Count: 4    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/nlog</b></font>
bin/nlog: Command not found.
UNIX> <font color=darkred><b>echo 16 | bin/nlogn</b></font>
N: 16     Count: 64    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/nsquared</b></font>
N: 16     Count: 256    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/all_i_j_pairs </b></font>
N: 16     Count: 120    Time: 0.000
UNIX> <font color=darkred><b>echo 16 | bin/two_to_the_n</b></font>
N: 16     Count: 65536    Time: 0.000
UNIX> <font color=darkred><b></b></font>
</pre>

In each program, the running time is going to be directly proportional
to <b>count</b>.  
So, what do the running times look like if you increase <b>n</b> to large
values?  To test this, I ran all of the programs with increasing values of <i>n</i>.
I quit either when <i>n</i> got really large (about 10<sup>15</sup>), or when 
the running time exceeded a minute.  You can see the data in the following
files (these are in the <b>data</b> directory</b>):
<p>
<center>
<table border=7 cellpadding=3>
<tr>
<td><a href=data/linear1.txt><b>linear1</b></a></td>
<td><a href=data/linear2.txt><b>linear2</b></a></td>
<td><a href=data/log.txt><b>log</b></a></td>
<td><a href=data/nlogn.txt><b>nlogn</b></a></td>
<td><a href=data/nsquared.txt><b>nsquared</b></a></td>
<td><a href=data/all_i_j_pairs.txt><b>all_i_j_pairs</b></a></td>
<td><a href=data/two_to_the_n.txt><b>two_to_the_n</b></a></td>
</tr></table>
</center>
</p>

It's pretty illustrative if you look at the last line of each file -- I've 
done some formatting:

<p><center><table border=3 cellpadding=3><td><pre>
log            N: 7500000000000000  Count: 53          Time: 0.000
linear1        N: 50000000000       Count: 50000000000 Time: 98.628
linear2        N: 25000000000       Count: 50000000000 Time: 99.176
nlogn          N: 2500000000        Count: 80000000000 Time: 167.156
nsquared       N: 250000            Count: 62500000000 Time: 125.680
all_i_j_pairs  N: 250000            Count: 31249875000 Time: 61.589
two_to_the_n   N: 35                Count: 34359738368 Time: 76.261
</pre></td></table></center><p>

Two quick observations: <b>log(n)</b> is <i>really</i> fast.  On the flip side,
<i>2<sup>n</sup></i> is <i>really</i> slow.  Below I show some graphs of the data.
The graphs all graph the same data; they just have different scales, so that you 
can do some visual comparisons:

<p><center><table border=3><td><img src=jpg/comparison-1.jpg width=800></td></table></center><p>

So, this shows what you'd think: 
<p>
<center>
<i><b>log(n)</b> < <b>n</b> < <b>2n</b> < <b>n*log(n)</b> < <b>all_pairs(n)</b> < <b>n*n</b> < <b>2<sup>n</sup></b> </i>
</center>
<p>
Perhaps its hard to gauge how much each is less than the other until you
see it.  Below I plot the same graph, but zoomed up a bit so you can get
a better feel for <b>n*log(n)</b> and <b>n*n</b>.
<p>

<hr>

<h2>Back to Big-O: Function comparison </h2>
Big-O notation tries to work on classifying functions.  The functions that
we care about are the running times of programs.  The first concept when
we deal with Big-O is comparing functions.  Basically, we will say that
one function <i>f(n)</i>is <b><i>greater</i></b> than another <i>g(n)</i>
if there is a value <i>x<sub>0</sub></i> so that for all <i>x >= x<sub>0</sub></i>:
<p><center>
<i> f(x) >= g(x)</i>
</center><p>

Put graphically, it means that after a certain point on the x axis,
as we go right, the curve for <i>f(n)</i> will always be higher than
<i>g(n)</i>.  Thus, given the graphs above, you can see that 
<i>n*n</i> is greater than
<i>n*log(n)</i>, which is greater than
<i>2n</i>, which is greater than
<i>n</i>, which is greater than
<i>log(n)</i>.

<p>
So, here are some functions:
<UL>
<LI> <i>a(n) = 1</i>
<LI> <i>b(n) = 100</i>
<LI> <i>c(n) = 6-n</i>
<LI> <i>d(n) = n</i>
<LI> <i>e(n) = 2n</i>
<LI> <i>f(n) = 2n-5</i>
<LI> <i>g(n) = n*n - 5000000000</i>
<LI> <i>h(n) = log(n)</i>
<LI> <i>i(n) = log(n) - 100</i>
<LI> <i>j(n) = n*log(n)-100</i>
</UL>

So, we can ask ourselves questions:  Is <i>b(n) > a(n)</i>?  Yes.  Why?
Because for any value of <i>n</i>, <i>b(n)</i> is 100, and <i>a(n)</i> is
1.  Therefore for any value of <i>n</i>, <i>b(n)</i> is greater than
<i>a(n)</i>.
<p>
That was easy.  How about <i>c(n)</i> and <i>b(n)</i>?  <i>b(n)</i> is
greater, because we can set <i>x<sub>0</sub></i> equal to 6: 
For any value of <i>x</i> greater than 6, <i>b(x)</i>
is 100 and <i>c(x)</i> is negative.  
<p>
Here's a total ordering of the above.  Make sure you can prove all 
of these to yourselves:
<p>
<center>
<i> g(n) > j(n) > e(n) > f(n) > d(n) > h(n) > i(n) > b(n) > a(n) > c(n)</i>
</center>
<p>
Some rules:  
<UL>
<LI>If <i>f(n)</i> and <i>g(n)</i> are both polynomials of 
degree <i>k</i>, then the lead coefficient defines which one is greater.
<LI>
If <i>f(n)</i> is a polynomial of degree <i>k</i> and <i>g(n)</i> is a 
polynomial of degree <i>l < k</i>, and both lead coefficients are 
positive, then <i>f(n) > g(n)</i>.
<LI> If <i>a*f(n) > b*g(n)</i> for all positive constants <i>a</i> and
<i>b</i>, then <i>c*f(n) + d*g(n) > e*f(n) + x*g(n)</i> if and only
if <i>c > e</i>.  For example, <i>20n*n - 100n > 19n*n + 60,000n</i>.
</UL>

<hr>
<h2>Big-O</h2>
Given the above, we can now define 
Big-O:
<p>
<center>
<i>T(N) = O(f(N))</i> if there exists a constant <i>c</i> such that
<i>c*f(N) >= T(N)</i>.
</center>
<p>

Given the definitions of <i>a(n)</i> through <i>j(n)</i> 
above:
<UL>
<LI> <i>a(n) = O(1)</i>.
<LI> <i>b(n) = O(1)</i>.  This is because we can set <i>c</i> to 101.
<LI> <i>b(n) = O(n)</i>.  Why?  Set <i>c</i> equal to 1 and <i>x<sub>0</sub></i> equal to 101. 
Then, <b>b(x)</b> will equal 100, and <i>cx</i> will be greater than 100.
<LI> <i>b(n) = O(n*n)</i>.  Set <i>c</i> equal to 1 -- you can demonstrate easily that <i>n*n > b(n)</i>.
<LI> <i>b(n) = O(a(n))</i>. Set <i>c</i> equal to 101, and this is equivalent to the second case above.
<LI> <i>a(n) = O(b(n))</i>. Set <i>c</i> equal to one.
<LI> <i>e(n) = O(n)</i>.  Set <i>c</i> equal to three and <i>x<sub>0</sub></i> equal to 1.  
<LI> <i>i(n) = O(log(n))</i>.  Set <i>c</i> equal to one and <i>x<sub>0</sub></i> equal to 1. 
<LI> <i>j(n) = O(n*log(n))</i>. Ditto.
<LI> <i>g(n) = O(n*n)</i>. Ditto.
</UL>

<hr>
<H2>Big Omega and Big Theta</h2>
Note that <i>O(f(N))</i> is an upper bound on <i>T(N)</i>.  That means
that <i>T(N)</i> is definitely not bigger than <i>f(n)</i>.  Similarly,
if <i>g(N) > f(N)</i> and <i>T(N) = O(f(N))</i>, then 
<i>T(N) = O(g(n))</i> too.  That's inconvenient.  Why?  Well, if a program's running time is linear
in the size of its input, then we'd like to say that the running time is <i>O(n)</i> and not 
<i>O(n<sup>2</sup>)</i>.  Unfortunately, it is both.
<p>
Big Omega and Bit Theta help make things more precise:
<p>
<UL>
<LI> <b>Big-Omega</b>: <i>T(N) = &Omega;(f(N))</i> if <i>f(N) = O(T(N))</i>.  While Big-O says that
<i>T(N)</i> is no bigger than a factor times <i>f(N)</i>, Big-Omega says that 
<i>T(N)</i> is no smaller than a factor times <i>f(N)</i>.
<p>
<LI> <b>Big-Theta</b>: <i>T(N) = &Theta;(f(N))</i> 
if <i>T(N) = O(f(N))</i> and <i>T(N) = &Omega;(f(N))</i>.  Big-Theta is the most precise of these
specifications.  It says that <i>T(N)</i> and <i>f(N)</i> are equivalent to constant factors of
each other.
</UL>

Let me give an example.  Suppose I have a program that takes <i>3n + 5</i> operations on an input of
size <i>n</i>.  We typically say that the program is <i>O(n)</i>.  That is clearly true 
(choose <i>c=4</i> and <i>x<sub>0</sub>=10</i>).  
However, as mentioned above, the program is also 
<i>O(n<sup>2</sup>)</i> 
(choose <i>c=1</i> and <i>x<sub>0</sub>=10</i>).  
Is it <i>O(1)</i>?  No -- there is no <i>c</i> such that <i>c &ge; 3n + 5</i>.
<p>
The program is <i>&Omega;(n)</i>: choose <i>c = 1</i> and <i>x<sub>0</sub>=1</i> (in other words, 
for any <i>x</i> &ge; 1, <i>3x+5 > x</i>).  However, it is not
<i>&Omega;(n<sup>2</sup>)</i>, because there is no <i>c</i> such that <i>c(3x+5) &ge; x<sup>2</sup></i>.  It is, however, 
<i>&Omega;(1)</i>: choose <i>c = 1</i> and <i>x<sub>0</sub>=1</i>  -- it's pretty easy to 
see that <i>3x + 5 > 1</i>.
<p>
Now, we can put this in terms of Big-Theta.
The program is 
<i>&Theta;(n)</i>, but not 
<i>&Theta;(n<sup>2</sup>)</i> or
<i>&Theta;(1)</i>.  
<p>
It is unfortunate that we as computer scientists quantify algorithms using Big-O rather than Big-Theta,
but it is a fact of life.  You need to know these definitions, and remember that most of the time, when
we say something is Big-O, in reality it is also Big-Theta, which is <i>much</i> more precise.
<hr>

At this point, I think that giving the 
<a href=http://en.wikipedia.org/wiki/Big_O_notation>Wikipedia page on Big-O</a>
a scan is a good idea.  This includes:
<p>
<UL>
<LI> The introduction.
<LI> The first two equations in the <b>Formal Definition</b>.
<LI> The <b>Example</b>.
<LI> The <b>Infinite asymptotics</b> section.
<LI> The <b>Equals Sign</b> section.
<LI> The <b>Orders of common functions</b> section (ignore the "L-notation" line).
<LI> The definitions of Big-O, Big-Omega and Big-Theta in the <b>Family of Bachmann-Landau notations</b> section.
<li> The text starting with "Aside from Big-O notation, ..." until the end of the section.
</UL>

<hr>
<h2>Using Big-O to Categorize</h2>

Although Big-O is laden with math, we use it to characterize the running times of
programs and algorithms.  The following Big-O characterizations are particularly useful
(and they are all Big-Theta as well, even though we don't say so).

<UL>
<LI> <i>O(1)</i>: This is called "constant time." Any time a program takes a constant
number of instructions, regardless of the input, it is constant time.  For example, 
determining the size of a vector is <i>O(1)</i>, regardless of the size of the vector, 
because the STL stores the size as part of the vector.
<p>
Appending an element to a list, deque or vector is <i>O(1)</i>.  Pushing an element onto the
front of a list or deque is <i>O(1)</i>.  Accessing any element of a vector or deque
is also <i>O(1)</i>.
<p>
Calling <b>begin()</b> or <b>end()</b> on any of the STL's data structures -- vector, deque,
list, set or map -- is <i>O(1)</i>.  Perhaps that is counterintuitive with a set or map,
but so be it.
<p>
<LI> <i>O(n)</i>: This is called "linear." This is when the program takes time that is
directly proportional to size of its input.  For example, creating a vector, list or 
deque with <i>n</i> elements is <i>O(n)</i>.  Of course, creating the vector is faster, 
but they are both linear, and their performance is directly proportional to <i>n</i>.
This is why we have the constant <i>c</i> in the definition of Big-O -- so that both 
of these are <i>O(n)</i>, even though the vector version is faster.
<p>
Traversing a set or map with an iterator is also <i>O(n)</i>.  This confuses students, 
because the other operations on sets or map involve logarithms.  So, memorize it.  Hopefully,
when you learn about AVL trees, you'll get a better feeling for that.
<p>
And finally, deleting an element or inserting an element in the front of a vector is
also <i>O(n)</i>.  This is why you don't want to use a vector for this operation.
<p>
<LI> <i>O(n<sup>2</sup>)</i>: This is called "quadratic time," and as the graphs above show,
it does not scale well.  In particular, when <i>n</i> hits a value of 10,000, <i>n<sup>2</sup></i>
gets pretty big (100,000,000).  The <b>all_i_j_pairs</b> program above has <b>count</b> equal
<i>(n+1)n/2</i>.  This is <i>O(n<sup>2</sup>)</i> as well.

<p>
<LI> <i>O(log(n))</i>: This is called "log time."   Now, you may ask "what base?"  The
answer is that the base doesn't matter.  Why is that?  Because:
<p>
<center>
    <i>log<sub>b</sub>(c) = log<sub>a</sub>(c) / log<sub>a</sub>(b)</i>.
</center>
<p>
Since <i>log<sub>a</sub>(b)</i> is a constant, for the purposes of Big-O, it doesn't matter.
That may seem confusing, so make it concrete.  If <i>a = log<sub>10</sub>(n)</i>, 
then <i>log<sub>2</sub>(n) = log<sub>2</sub>(10)*a</i>.  Since <i>log<sub>2</sub>(10)</i> is
a constant (a little greater than three), for the purposes of Big-O, logarithms in base 10 and
base 2 are equivalent.
<p>
Insertion, deletion, and finding elements in sets and maps are <i>O(log(n))</i> operations.

<p>
<LI> <i>O(n*log(n))</i>: This is called "N log N."   Creating maps and sets are
<i>O(n*log(n))</i> operations.  So is sorting a vector with <i>n</i> elements.
</UL>
<hr>
<h2>Two Big-O Proofs</h2>

You are not responsible for proofs like this, but it's not a bad idea to see them:
<p>
Is <i>n*n + n + 1 = O(n*n)?</i>  See the following <a href=latex/proof.pdf>PDF file</a> 
for a proof.

<p>

Generalizing, is <i>an*n + bn + d = O(n*n)</i> for <i>a,b,d > 1</i> and <i>b > d</i>? 
See the following <a href=latex/proof2.pdf>PDF file</a> for a proof.
